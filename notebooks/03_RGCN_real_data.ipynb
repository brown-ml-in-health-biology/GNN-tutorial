{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Applying RGCN model to real-world data**\n",
        "After experimenting with a simple, synthetic dataset to understand the fundamentals, we now apply the RGCN model to a real-world biomedical dataset!\n",
        "\n",
        "In this notebook, we:\n",
        "1. Load real-world biomedical dataset.\n",
        "2. Convert it into a PyTorch Geometric `Data` object.\n",
        "3. Stratify-split the edges by relation type into train/val/test.\n",
        "4. Train and evaluate an RGCN for link prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import RGCNConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. About the Pharmacotherapy Dataset\n",
        "\n",
        "The PharmacotherapyDB dataset links diseases/conditions to drug treatments and specify the type of relationship (for example, \"treats\" or \"palliates\" or \"neither treats nor palliates\").\n",
        "\n",
        "This dataset has the following structure:\n",
        "- The column doid_id is the disease identifier (often from the Disease Ontology), such as DOID:1234.\n",
        "- The column drugbank_id is the drug identifier such as DB00123 (DrugBank format).\n",
        "- The column Y indicates the relation between them, for example \"treats\" (0 for \"treats\", 1 for \"palliates\", 2 \"neither treats nor palliates\") \n",
        "\n",
        "From this dataset, we will construct a graph where each disease and each drug is a node, and an edge connects them according to the specified Y relationship. We can then train a Relational Graph Convolutional Network (RGCN) to link prediction, where the goal is to predict missing or potential relationships between nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b07c917e",
      "metadata": {},
      "source": [
        "## 2. Data structure and loading\n",
        "This function reads the CSV and creates:\n",
        "- A mapping of entity strings to integer indices (for both `doid_id` and `drugbank_id`).\n",
        "- A mapping of relation strings to integer relation indices.\n",
        "- A `Data` object from PyG, with:\n",
        "  - `data.x`: random embeddings for each unique entity (size `[num_nodes, embedding_dim]`).\n",
        "  - `data.edge_index`: shape `[2, num_edges]`.\n",
        "  - `data.edge_type`: shape `[num_edges]` (identifies each edge's relation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "def load_pharma_data(\n",
        "    csv_file,\n",
        "    doid_col='doid_id',\n",
        "    drug_col='drugbank_id',\n",
        "    rel_col='Y',\n",
        "    embedding_dim=32\n",
        "):\n",
        "    \"\"\"\n",
        "    Reads the Pharmacotherapy CSV (with columns [doid_col, drug_col, rel_col])\n",
        "    and produces a PyG Data object.\n",
        "    \"\"\"\n",
        "    # 1) Read CSV\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # 2) Gather entities & relations\n",
        "    unique_entities = set()\n",
        "    unique_relations = set()\n",
        "    edges = []\n",
        "\n",
        "    for row in df.itertuples(index=False):\n",
        "        doid_id = getattr(row, doid_col)\n",
        "        drug_id = getattr(row, drug_col)\n",
        "        rel = getattr(row, rel_col)\n",
        "\n",
        "        unique_entities.add(doid_id)\n",
        "        unique_entities.add(drug_id)\n",
        "        unique_relations.add(rel)\n",
        "\n",
        "        edges.append((doid_id, drug_id, rel))\n",
        "\n",
        "    # 3) Sort and map to indices\n",
        "    entity_list = sorted(list(unique_entities))\n",
        "    entity_to_idx = {ent: i for i, ent in enumerate(entity_list)}\n",
        "\n",
        "    relation_list = sorted(list(unique_relations))\n",
        "    relation_to_idx = {r: i for i, r in enumerate(relation_list)}\n",
        "\n",
        "    # 4) Build edge_index & edge_type\n",
        "    edge_index_list = []\n",
        "    edge_type_list = []\n",
        "\n",
        "    for (doid_id, drug_id, rel) in edges:\n",
        "        src = entity_to_idx[doid_id]\n",
        "        dst = entity_to_idx[drug_id]\n",
        "        edge_index_list.append([src, dst])\n",
        "        edge_type_list.append(relation_to_idx[rel])\n",
        "\n",
        "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
        "    edge_type = torch.tensor(edge_type_list, dtype=torch.long)\n",
        "\n",
        "    # 5) Random embeddings for each node\n",
        "    num_nodes = len(entity_to_idx)\n",
        "    x = torch.randn((num_nodes, embedding_dim), dtype=torch.float)\n",
        "\n",
        "    # 6) Create PyG Data\n",
        "    data = Data(\n",
        "        x=x,\n",
        "        edge_index=edge_index,\n",
        "        edge_type=edge_type\n",
        "    )\n",
        "\n",
        "    return data, entity_to_idx, relation_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Stratified Splitting of Edges\n",
        "\n",
        "In a multi-relation graph, some relation types might be much more common than others. A purely random\n",
        "split could end up with certain relation types over-represented in one set and under-represented\n",
        "in another. **Stratification** ensures that each relation type is present in similar proportions \n",
        "across **train, val, and test** sets, helping prevent issues where a model never sees certain\n",
        "relation types during training or sees them only in the test set.\n",
        "\n",
        "We'll split 80% train, 10% val, 10% test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "def stratified_split_edges(data, test_size=0.2, val_size=0.1, random_seed=42):\n",
        "    \"\"\"\n",
        "    Splits data.edge_index/data.edge_type by relation type.\n",
        "    80% train, 10% val, 10% test by default.\n",
        "    Returns train/val/test edges + their relation types.\n",
        "    \"\"\"\n",
        "    edge_index_np = data.edge_index.cpu().numpy()\n",
        "    edge_type_np = data.edge_type.cpu().numpy()\n",
        "\n",
        "    num_edges = edge_index_np.shape[1]\n",
        "    all_edge_ids = np.arange(num_edges)\n",
        "    all_edge_labels = edge_type_np  # for stratification\n",
        "\n",
        "    # Step 1: train vs. temp split\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        all_edge_ids,\n",
        "        all_edge_labels,\n",
        "        test_size=test_size,\n",
        "        stratify=all_edge_labels,\n",
        "        random_state=random_seed\n",
        "    )\n",
        "\n",
        "    # Step 2: val vs. test from X_temp\n",
        "    # if test_size=0.2 => X_temp is 20%. We want val=10%, test=10% overall\n",
        "    # that means half of X_temp is val, half is test\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp,\n",
        "        y_temp,\n",
        "        test_size=0.5,  # 50% of the temp => 10% overall\n",
        "        stratify=y_temp,\n",
        "        random_state=random_seed\n",
        "    )\n",
        "\n",
        "    def to_edge_tensors(edge_ids):\n",
        "        sub_edge_index = torch.tensor(edge_index_np[:, edge_ids], dtype=torch.long)\n",
        "        sub_edge_type = torch.tensor(edge_type_np[edge_ids], dtype=torch.long)\n",
        "        return sub_edge_index, sub_edge_type\n",
        "\n",
        "    train_edge_index, train_edge_type = to_edge_tensors(X_train)\n",
        "    val_edge_index, val_edge_type = to_edge_tensors(X_val)\n",
        "    test_edge_index, test_edge_type = to_edge_tensors(X_test)\n",
        "\n",
        "    return (train_edge_index, train_edge_type,\n",
        "            val_edge_index, val_edge_type,\n",
        "            test_edge_index, test_edge_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Example RGCNLinkPredictor\n",
        "If you already defined this in your original tutorial, you can skip this cell.\n",
        "Otherwise, here's a minimal version of an RGCN-based link predictor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "class RGCNLinkPredictor(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_nodes,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        num_relations,\n",
        "        num_layers=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_nodes = num_nodes\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_relations = num_relations\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Build RGCN layers\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(RGCNConv(in_channels, out_channels, num_relations, num_bases=4))\n",
        "\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(RGCNConv(out_channels, out_channels, num_relations, num_bases=4))\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index, edge_type):\n",
        "        # x: [num_nodes, in_channels]\n",
        "        # edge_index: [2, E]\n",
        "        # edge_type: [E]\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index, edge_type)\n",
        "            x = F.relu(x)\n",
        "        return x  # node embeddings\n",
        "\n",
        "    def predict(self, node_embeddings, edge_index):\n",
        "        # Typically, for link prediction, we do a dot-product\n",
        "        # edge_index: [2, E]\n",
        "        src = node_embeddings[edge_index[0]]  # [E, out_channels]\n",
        "        dst = node_embeddings[edge_index[1]]  # [E, out_channels]\n",
        "        score = (src * dst).sum(dim=-1)  # dot product => shape [E]\n",
        "        # Return a sigmoid to get a score between 0 and 1\n",
        "        return torch.sigmoid(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Negative Sampling & Training/Eval Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "def negative_sampling(num_neg_samples, num_nodes, device=device):\n",
        "    i = torch.randint(0, num_nodes, (num_neg_samples,), device=device)\n",
        "    j = torch.randint(0, num_nodes, (num_neg_samples,), device=device)\n",
        "    return torch.stack([i, j], dim=0)\n",
        "\n",
        "def train(model, optimizer, data, train_edge_index, train_edge_type):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    node_embeddings = model(data.x, train_edge_index, train_edge_type)\n",
        "\n",
        "    # Positive\n",
        "    pos_score = model.predict(node_embeddings, train_edge_index)\n",
        "\n",
        "    # Negative\n",
        "    neg_edge_index = negative_sampling(\n",
        "        num_neg_samples=train_edge_index.size(1),\n",
        "        num_nodes=data.num_nodes,\n",
        "        device=data.x.device\n",
        "    )\n",
        "    neg_score = model.predict(node_embeddings, neg_edge_index)\n",
        "\n",
        "    # Link prediction loss\n",
        "    loss_pos = -torch.log(pos_score + 1e-15).mean()\n",
        "    loss_neg = -torch.log(1. - neg_score + 1e-15).mean()\n",
        "    loss = loss_pos + loss_neg\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(edge_index, edge_type, threshold=0.5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Obtain node embeddings\n",
        "        node_embeddings = model(data.x, edge_index, edge_type)\n",
        "\n",
        "        # Positive (real) edge scores\n",
        "        pos_score = model.predict(node_embeddings, edge_index)\n",
        "\n",
        "        # Negative edges (sample the same number as positive)\n",
        "        neg_edge_index = negative_sampling(edge_index.size(1), data.num_nodes)\n",
        "        neg_score = model.predict(node_embeddings, neg_edge_index)\n",
        "\n",
        "    # Ground-truth labels: 1 for real edges, 0 for negatives\n",
        "    y_true = torch.cat([\n",
        "        torch.ones(pos_score.size(0), device=device),\n",
        "        torch.zeros(neg_score.size(0), device=device)\n",
        "    ], dim=0)\n",
        "\n",
        "    # Combine predicted scores\n",
        "    y_scores = torch.cat([pos_score, neg_score], dim=0)\n",
        "\n",
        "    # Binarize predictions\n",
        "    y_pred = (y_scores >= threshold).float()\n",
        "\n",
        "    # Convert to NumPy\n",
        "    y_true_np = y_true.cpu().numpy()\n",
        "    y_pred_np = y_pred.cpu().numpy()\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(y_true_np, y_pred_np)\n",
        "    f1 = f1_score(y_true_np, y_pred_np)\n",
        "    kappa = cohen_kappa_score(y_true_np, y_pred_np)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'kappa': kappa\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Main Flow\n",
        "\n",
        "Load data, do a stratified split, train the RGCN, and evaluate performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data(x=[647, 16], edge_index=[2, 1136], edge_type=[1136])\n",
            "Number of nodes: 647\n",
            "Number of edges: 1136\n",
            "Example of data.x shape: torch.Size([647, 16])\n",
            "Epoch 5 | Train Loss: 1.4073\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 10 | Train Loss: 1.3832\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 15 | Train Loss: 1.3807\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 20 | Train Loss: 1.3725\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 25 | Train Loss: 1.3631\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 30 | Train Loss: 1.3624\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 35 | Train Loss: 1.3426\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 40 | Train Loss: 1.3414\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 45 | Train Loss: 1.3148\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 50 | Train Loss: 1.3166\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 55 | Train Loss: 1.3013\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 60 | Train Loss: 1.3022\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 65 | Train Loss: 1.2867\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 70 | Train Loss: 1.2742\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 75 | Train Loss: 1.2666\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 80 | Train Loss: 1.2495\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 85 | Train Loss: 1.2396\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 90 | Train Loss: 1.2566\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 95 | Train Loss: 1.2311\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 100 | Train Loss: 1.2245\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 105 | Train Loss: 1.2421\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 110 | Train Loss: 1.2412\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 115 | Train Loss: 1.2010\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 120 | Train Loss: 1.2053\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 125 | Train Loss: 1.2059\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 130 | Train Loss: 1.2178\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 135 | Train Loss: 1.2094\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 140 | Train Loss: 1.1940\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 145 | Train Loss: 1.2003\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 150 | Train Loss: 1.1986\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 155 | Train Loss: 1.1892\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 160 | Train Loss: 1.1750\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 165 | Train Loss: 1.1853\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 170 | Train Loss: 1.1809\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 175 | Train Loss: 1.1638\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 180 | Train Loss: 1.1730\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 185 | Train Loss: 1.1746\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 190 | Train Loss: 1.1706\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 195 | Train Loss: 1.1862\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Epoch 200 | Train Loss: 1.1781\n",
            "Validation: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n",
            "Test: {'accuracy': 0.5, 'f1': 0.6666666666666666, 'kappa': np.float64(0.0)}\n"
          ]
        }
      ],
      "source": [
        "csv_file = \"/Users/ioanna/Documents/Projects/singh-lab/courses/brown-ml-in-health-biology/GNN-tutorial/data/pharmacotherapyDB.csv\"\n",
        "\n",
        "# Step A: Load & Preprocess\n",
        "data, entity_to_idx, relation_to_idx = load_pharma_data(\n",
        "    csv_file,\n",
        "    doid_col='doid_id',\n",
        "    drug_col='drugbank_id',\n",
        "    rel_col='Y',\n",
        "    embedding_dim=16  # play around with this value\n",
        ")\n",
        "print(data)\n",
        "print(\"Number of nodes:\", data.num_nodes)\n",
        "print(\"Number of edges:\", data.num_edges)\n",
        "print(\"Example of data.x shape:\", data.x.shape)\n",
        "\n",
        "# Step B: Stratified Split\n",
        "(\n",
        "    train_edge_index,\n",
        "    train_edge_type,\n",
        "    val_edge_index,\n",
        "    val_edge_type,\n",
        "    test_edge_index,\n",
        "    test_edge_type\n",
        ") = stratified_split_edges(data, test_size=0.2, val_size=0.1, random_seed=42)\n",
        "\n",
        "# Step C: Build the RGCN model\n",
        "num_nodes = data.num_nodes\n",
        "num_relations = int(torch.max(data.edge_type)) + 1\n",
        "in_channels = data.x.size(1)\n",
        "out_channels = 8\n",
        "\n",
        "model = RGCNLinkPredictor(\n",
        "    num_nodes=num_nodes,\n",
        "    in_channels=in_channels,\n",
        "    out_channels=out_channels,\n",
        "    num_relations=num_relations,\n",
        "    num_layers=2\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Step D: Move data + edges to the device\n",
        "data = data.to(device)\n",
        "train_edge_index = train_edge_index.to(device)\n",
        "train_edge_type = train_edge_type.to(device)\n",
        "val_edge_index   = val_edge_index.to(device)\n",
        "val_edge_type   = val_edge_type.to(device)\n",
        "test_edge_index  = test_edge_index.to(device)\n",
        "test_edge_type  = test_edge_type.to(device)\n",
        "\n",
        "# Step E: Train\n",
        "EPOCHS = 200\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train(model, optimizer, data, train_edge_index, train_edge_type)\n",
        "    \n",
        "    if epoch % 5 == 0:  # Evaluate every 5 epochs\n",
        "        val_metrics = evaluate(val_edge_index, val_edge_type, threshold=0.5)\n",
        "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f}\")\n",
        "        print(\"Validation:\", val_metrics)\n",
        "\n",
        "# Step F: Final test evaluation\n",
        "test_metrics = evaluate(test_edge_index, test_edge_type, threshold=0.5)\n",
        "print(\"Test:\", test_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76c4a9ad",
      "metadata": {},
      "source": [
        "Note: While training loss decreases, performance on validation and test sets remains flat. This usually indicates that the node representations are not informative enough for the task.\n",
        "\n",
        "Where do node representations (`data.x`) come from?\n",
        "\n",
        "- They can be raw features (e.g., gene expression levels, molecular fingerprints).\n",
        "- They might be pretrained embeddings (e.g., using a language model or a separate encoder).\n",
        "- They could be one-hot encodings (if no features are available)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7786364a",
      "metadata": {},
      "source": [
        "## Homework: Improving Node Representations!\n",
        "\n",
        "Currently, our node features (`data.x`) are initialized randomly:\n",
        "`x = torch.randn((num_nodes, embedding_dim), dtype=torch.float)`\n",
        "\n",
        "While this works for testing the pipeline, **random embeddings do not capture any domain knowledge**.\n",
        "This is why our model is not improving on validation/test performance.\n",
        "\n",
        "## Try the following to improve your node features:\n",
        "\n",
        "### 1. One-Hot Encoding\n",
        "Encode each node with a one-hot vector (unique to each entity).\n",
        "This gives the model a stable identity feature for each node.\n",
        "(*Tip: use torch.eye(num_nodes)*)\n",
        "\n",
        "### 2. Pretrained Embeddings\n",
        "- For drugs: Use molecular fingerprints (e.g., PubChem, Morgan fingerprints) or text-based embeddings (e.g., BioBERT).\n",
        "- For diseases: Use text-based embeddings (e.g., BioBERT, node2vec on disease ontologies).\n",
        "You'll need to map each entity ID to its corresponding vector.\n",
        "\n",
        "### 3. Feature Concatenation\n",
        "Combine multiple feature types — e.g., one-hot + pretrained — to increase richness.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "02_RGCN_tutorial_updated.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
