{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979cb6fb",
   "metadata": {},
   "source": [
    "# Introduction to Graph Neural Networks (GNNs)\n",
    "\n",
    "In this notebook, we cover the basics of Graph Neural Networks (GNNs), focusing primarily on the **Graph Convolutional Network (GCN)**. We’ll see the general message-passing idea and walk through a small example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6625b9",
   "metadata": {},
   "source": [
    "## 0. Why do people use GNNs?\n",
    "\n",
    "Many real-world datasets have rich relational structures (e.g., social networks, citation networks, molecular graphs, knowledge graphs). Traditional fully-connected or convolutional networks aren’t designed to leverage such adjacency information explicitly. **GNNs** fill this gap by incorporating the graph’s structure in each forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed8913f",
   "metadata": {},
   "source": [
    "## 1. Introduction to Graph Neural Networks\n",
    "\n",
    "In this tutorial, we will explore **Graph Neural Networks (GNNs)**, which are specialized neural networks designed to handle data represented as graphs (nodes + edges). We will cover:\n",
    "1. The high-level idea behind GNNs (*Message Passing*).\n",
    "2. The **Graph Convolutional Network (GCN)** architecture.\n",
    "3. A simple example of GCN-based node classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f36b5",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "Below, we load common libraries needed for our experiments. Make sure you have installed:\n",
    "- `torch` (PyTorch)\n",
    "- `numpy`\n",
    "\n",
    "You can install them via:\n",
    "```bash\n",
    "pip install torch numpy\n",
    "```\n",
    "Then in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e18094",
   "metadata": {},
   "source": [
    "## 3. Graph Notation & Message Passing\n",
    "\n",
    "### 3.1 Graph Notation\n",
    "\n",
    "A graph **G = (V, E)** has:\n",
    "- **V**: a set of nodes (or vertices), |V| = N.\n",
    "- **E**: a set of edges, each edge (i, j) connects two nodes.\n",
    "\n",
    "We often store this information in an **adjacency matrix** $ A \\in \\{0,1\\}^{N\\times N}$. Also, each node i can have a feature vector $x_i \\in \\mathbb{R}^d$. Collectively, these are often stored as a matrix $X \\in \\mathbb{R}^{N \\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd8591",
   "metadata": {},
   "source": [
    "### 3.2 Message Passing Paradigm\n",
    "\n",
    "The core idea behind GNNs is **message passing**:\n",
    "\n",
    "1. Each node begins with an initial representation (often just its features):  \n",
    "   $h_i^{(0)} = x_i$\n",
    "\n",
    "2. At each layer `k`, every node aggregates information from its neighbors to form a new representation $h_i^{(k)}$.\n",
    "\n",
    "3. After multiple layers, each node’s representation captures information from a broader neighborhood in the graph.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$\n",
    "h_i^{(k)} = \\text{UPDATE}\\left(h_i^{(k-1)}, \\text{AGGREGATE}\\left(\\{ h_j^{(k-1)} : j \\in \\mathcal{N}(i) \\}\\right)\\right)\n",
    "$\n",
    "\n",
    "where $ \\mathcal{N}(i) $ denotes the set of neighbors of node $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcabffd9",
   "metadata": {},
   "source": [
    "## 4. Graph Convolutional Network (GCN)\n",
    "\n",
    "One of the earliest and most widely used GNN variants is the **Graph Convolutional Network (GCN)** by Kipf & Welling (ICLR 2017). The layer update is typically written in matrix form:\n",
    "\n",
    "$\n",
    "H^{(k+1)} = \\sigma\\left(\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(k)} W^{(k)}\\right)\n",
    "$\n",
    "\n",
    "where:\n",
    "- $\\tilde{A} = A + I$ (we add self-connections along the diagonal),\n",
    "- $\\tilde{D}$ is the diagonal degree matrix of $\\tilde{A}$,\n",
    "- $H^{(k)} \\in \\mathbb{R}^{N \\times d_k}$ is the matrix of node embeddings in the $k$-th layer,\n",
    "- $W^{(k)}$ is a trainable weight matrix,\n",
    "- $\\sigma$ is a non-linear activation (e.g., ReLU).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cadfe7",
   "metadata": {},
   "source": [
    "### 4.1 Minimal GCN Implementation\n",
    "\n",
    "Below, we define two classes:\n",
    "- **`SimpleGCNLayer`**: a single GCN layer that computes $\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} X W$.\n",
    "- **`SimpleGCN`**: a 2-layer GCN network (for illustration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2f7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGCNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SimpleGCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"x: (N, d_in), adj: (N, N) adjacency with self-loops added\"\"\"\n",
    "        # Compute the degree of each node\n",
    "        degree = torch.sum(adj, dim=1, keepdim=True)\n",
    "        degree_inv_sqrt = torch.pow(degree, -0.5)\n",
    "        degree_inv_sqrt[torch.isinf(degree_inv_sqrt)] = 0.0  # prevent inf\n",
    "\n",
    "        # Normalize adjacency: D^{-1/2} * A * D^{-1/2}\n",
    "        adj_norm = degree_inv_sqrt * adj * degree_inv_sqrt.transpose(0, 1)\n",
    "\n",
    "        # GCN operation\n",
    "        x = torch.mm(adj_norm, x)            # (N, d_in)\n",
    "        x = self.linear(x)                   # (N, d_out)\n",
    "        return x\n",
    "\n",
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(SimpleGCN, self).__init__()\n",
    "        self.gcn1 = SimpleGCNLayer(in_features, hidden_features)\n",
    "        self.gcn2 = SimpleGCNLayer(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.gcn1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.gcn2(x, adj)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c4e645",
   "metadata": {},
   "source": [
    "### 4.2 Example: Node Classification (Toy Data)\n",
    "\n",
    "We’ll define a small graph of 5 nodes, construct a toy adjacency matrix, and attempt a simple classification (2 classes). \n",
    "In reality, you’d apply this to bigger datasets, but the process is the same. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88282a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy data\n",
    "N = 5              # number of nodes\n",
    "d_in = 4           # dimensionality of input features\n",
    "x_toy = torch.rand(N, d_in)\n",
    "\n",
    "# Toy adjacency matrix: connect nodes in a chain + self-loops\n",
    "adj_toy = torch.zeros(N, N)\n",
    "for i in range(N - 1):\n",
    "    adj_toy[i, i+1] = 1\n",
    "    adj_toy[i+1, i] = 1\n",
    "for i in range(N):\n",
    "    adj_toy[i, i] = 1\n",
    "\n",
    "# Labels for each node (0 or 1)\n",
    "labels = torch.tensor([0, 1, 1, 0, 1])\n",
    "\n",
    "# Create model\n",
    "model = SimpleGCN(in_features=d_in, hidden_features=8, out_features=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(x_toy, adj_toy)  # shape (N, 2)\n",
    "    loss = criterion(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Final logits\n",
    "print(\"\\nFinal logits:\\n\", logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef5172",
   "metadata": {},
   "source": [
    "Not very interesting for just 5 nodes, but the pipeline is the same for bigger graphs! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e8829",
   "metadata": {},
   "source": [
    "## 5. Conclusion & Further Reading\n",
    "\n",
    "In this tutorial, we covered:\n",
    "- **Graph Notation & Message Passing**\n",
    "- **Graph Convolutional Network (GCN)** theory + example\n",
    "\n",
    "**Further Reading**:\n",
    "- Kipf & Welling (2017) \"Semi-Supervised Classification with GCNs\"\n",
    "- Velickovic et al. (2018) \"Graph Attention Networks\"\n",
    "- Hamilton et al. (2017) \"GraphSAGE\"\n",
    "- [PyTorch Geometric Documentation](https://pytorch-geometric.readthedocs.io/)\n",
    "- [DGL Documentation](https://www.dgl.ai/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2803a3",
   "metadata": {},
   "source": [
    "## 6. Advanced Topics\n",
    "\n",
    "Some popular extensions to GCN include:\n",
    "1. **Graph Attention Networks (GAT)**: Use attention to weight neighbors differently.\n",
    "2. **GraphSAGE**: Sample fixed-size neighborhoods for large-scale graphs.\n",
    "3. **Heterogeneous Graphs**: Different node types and different edge types.\n",
    "4. **Scaling GNNs**: Efficient training on huge graphs (mini-batching, sampling, etc.).\n",
    "\n",
    "_(R-GCN, which handles multiple relation types, is covered in a separate tutorial.)_"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "name": "gnn_intro_tutorial"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
