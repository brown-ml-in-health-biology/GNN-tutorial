{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Why do people use GNNs?\n",
    "Many real-world datasets have rich relational structures (e.g., social networks, citation networks, molecular graphs, knowledge graphs). \n",
    "Traditional fully-connected or convolutional networks aren’t designed to this graph structure. \n",
    "GNNs fill this gap by explicitly using the graph’s adjacency information during the forward pass.\n",
    "\n",
    "# 1. Introduction to Graph Neural Networks\n",
    "\n",
    "In this tutorial, we will explore **Graph Neural Networks (GNNs)**, \n",
    "which are specialized neural networks designed to handle data represented as graphs (nodes + edges). \n",
    "We will cover:\n",
    "\n",
    "1. The high-level idea behind GNNs (Message Passing).\n",
    "2. The **Graph Convolutional Network (GCN)** architecture.\n",
    "3. A simple example of GCN-based node classification.\n",
    "4. An introduction to **Relational GCN (R-GCN)** for multi-relational data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "Below, we load common libraries needed for our experiments. Make sure you have installed:\n",
    "- `torch` (PyTorch)\n",
    "- `numpy`\n",
    "\n",
    "You can install them via `pip install torch numpy`.\n",
    "\n",
    "*(If you have `torch-geometric`, we’ll discuss how to use it in an optional section. But it’s not required for the core tutorial.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Graph Notation & Message Passing\n",
    "\n",
    "### 3.1 Graph Notation\n",
    "A graph **G = (V, E)** has:\n",
    "\n",
    "- **V**: a set of nodes (or vertices), $|V| = N$.\n",
    "- **E**: a set of edges. Each edge is a connection $(i, j)$ between two nodes.\n",
    "\n",
    "We often store this information in an **adjacency matrix** $A \\in \\{0,1\\}^{N \\times N}$, where:\n",
    "\n",
    "$$\n",
    "A_{ij} = \\begin{cases}\n",
    "1 & \\text{if there is an edge between node } i \\text{ and node } j, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We also assume each node $i$ has a feature vector $x_i \\in \\mathbb{R}^d$. \n",
    "Collectively, we can store them in a matrix $X \\in \\mathbb{R}^{N \\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Message Passing Paradigm\n",
    "\n",
    "The core idea behind GNNs is **message passing**:\n",
    "\n",
    "1. Each node begins with an initial representation (often just its input features): $h_i^{(0)} = x_i$.\n",
    "2. At each layer (or step) $k$, every node aggregates information from its neighbors to form a new representation $h_i^{(k)}$.\n",
    "3. After multiple layers, each node’s representation captures information from a broader neighborhood in the graph.\n",
    "\n",
    "The typical update rule is:\n",
    "\n",
    "$$\n",
    "h_i^{(k)} = \\text{UPDATE}\\Bigl(h_i^{(k-1)}, \\text{AGGREGATE}\\bigl(\\{h_j^{(k-1)} : j \\in \\mathcal{N}(i)\\}\\bigr)\\Bigr),\n",
    "$$\n",
    "\n",
    "where $\\mathcal{N}(i)$ is the set of neighbors of $i$. \n",
    "Different GNN variants define different **AGGREGATE** and **UPDATE** functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Graph Convolutional Network (GCN)\n",
    "\n",
    "One of the earliest and most widely used GNN variants is the **Graph Convolutional Network (GCN)** by Kipf & Welling (ICLR 2017). The layer update is typically written in matrix form:\n",
    "\n",
    "$$\n",
    "H^{(k+1)} = \\sigma\\left(\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(k)} W^{(k)}\\right),\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\tilde{A} = A + I$ (we add self-connections along the diagonal).\n",
    "- $\\tilde{D}$ is the diagonal degree matrix of $\\tilde{A}$, i.e., $\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$.\n",
    "- $H^{(k)} \\in \\mathbb{R}^{N \\times d_k}$ is the matrix of node embeddings in the $k$-th layer.\n",
    "- $W^{(k)} \\in \\mathbb{R}^{d_k \\times d_{k+1}}$ is a trainable weight matrix.\n",
    "- $\\sigma$ is a non-linear activation (e.g., ReLU).\n",
    "\n",
    "Conceptually, **GCN** normalizes neighbor features, applies a linear transform, and then uses a non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Minimal GCN Implementation\n",
    "\n",
    "Below, we define two classes:\n",
    "\n",
    "- **`SimpleGCNLayer`**: a single GCN layer that computes $\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} X W$.\n",
    "- **`SimpleGCN`**: a 2-layer GCN network (just for illustration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGCNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SimpleGCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"x: (N, d_in), adj: (N, N) adjacency with self-loops added\"\"\"\n",
    "        # Compute the degree of each node\n",
    "        degree = torch.sum(adj, dim=1, keepdim=True)\n",
    "        degree_inv_sqrt = torch.pow(degree, -0.5)\n",
    "        degree_inv_sqrt[torch.isinf(degree_inv_sqrt)] = 0.0  # prevent inf\n",
    "\n",
    "        # Normalize adjacency: D^{-1/2} * A * D^{-1/2}\n",
    "        adj_norm = degree_inv_sqrt * adj * degree_inv_sqrt.transpose(0, 1)\n",
    "\n",
    "        # GCN operation\n",
    "        x = torch.mm(adj_norm, x)            # (N, d_in)\n",
    "        x = self.linear(x)                   # (N, d_out)\n",
    "        return x\n",
    "\n",
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(SimpleGCN, self).__init__()\n",
    "        self.gcn1 = SimpleGCNLayer(in_features, hidden_features)\n",
    "        self.gcn2 = SimpleGCNLayer(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.gcn1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.gcn2(x, adj)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Example: Node Classification (Toy Data)\n",
    "\n",
    "We’ll define a small graph of 5 nodes, construct a toy adjacency matrix, and attempt a simple classification (2 classes). In reality, you’d apply this to a bigger dataset, but the process is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.7042\n",
      "Epoch 20, Loss: 0.6966\n",
      "Epoch 30, Loss: 0.6937\n",
      "Epoch 40, Loss: 0.6922\n",
      "Epoch 50, Loss: 0.6912\n",
      "\n",
      "Final logits:\n",
      " tensor([[-0.0852, -0.0619],\n",
      "        [-0.0840, -0.0616],\n",
      "        [-0.0836, -0.0615],\n",
      "        [-0.0834, -0.0614],\n",
      "        [-0.0833, -0.0614]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Toy data\n",
    "N = 5              # number of nodes\n",
    "d_in = 4           # dimensionality of input features\n",
    "x_toy = torch.rand(N, d_in)\n",
    "\n",
    "# Toy adjacency matrix: connect nodes in a chain + self-loops\n",
    "adj_toy = torch.zeros(N, N)\n",
    "for i in range(N - 1):\n",
    "    adj_toy[i, i+1] = 1\n",
    "    adj_toy[i+1, i] = 1\n",
    "# Add self-loops\n",
    "for i in range(N):\n",
    "    adj_toy[i, i] = 1\n",
    "\n",
    "# Labels for each node (0 or 1)\n",
    "labels = torch.tensor([0, 1, 1, 0, 1])\n",
    "\n",
    "# Create model\n",
    "model = SimpleGCN(in_features=d_in, hidden_features=8, out_features=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(x_toy, adj_toy)  # shape (N, 2)\n",
    "    loss = criterion(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Final logits\n",
    "print(\"\\nFinal logits:\\n\", logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very interesting for just 5 nodes, but the pipeline is the same for bigger graphs! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Relational Graph Convolutional Network (R-GCN)\n",
    "\n",
    "## 5.1 Why R-GCN?\n",
    "\n",
    "A typical GCN assumes a single type of relationship between nodes (i.e., all edges are the same). However, in many real-world graphs, edges have different types or relations (e.g., in knowledge graphs, we have different predicates). **Relational GCN** (R-GCN) handles multiple relation types by using separate parameters for each relation.\n",
    "\n",
    "## 5.2 Formulation\n",
    "\n",
    "For each node $i$, an R-GCN layer is defined as:\n",
    "\n",
    "$$\n",
    "h_i^{(k+1)} = \\sigma\\left( \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_r(i)} \\frac{1}{c_{i,r}} W_r^{(k)} h_j^{(k)} + W_0^{(k)} h_i^{(k)} \\right),\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathcal{R}$ is the set of relation types,\n",
    "- $W_r^{(k)}$ is a weight matrix for relation $r$,\n",
    "- $\\mathcal{N}_r(i)$ is the set of neighbors of node $i$ under relation $r$,\n",
    "- $c_{i,r}$ is a normalization constant (e.g., the size of $\\mathcal{N}_r(i)$),\n",
    "- $\\sigma$ is a non-linear activation function (e.g., ReLU).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Minimal R-GCN Implementation\n",
    "Below, we’ll implement a toy version of R-GCN with two relations. We store the adjacency info in a dictionary: `{relation_id: adjacency_matrix}`. Then we sum the contributions from each relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_relations):\n",
    "        super(RGCNLayer, self).__init__()\n",
    "        self.num_relations = num_relations\n",
    "        # Each relation has its own linear transform\n",
    "        self.weight = nn.ModuleList([\n",
    "            nn.Linear(in_features, out_features, bias=False)\n",
    "            for _ in range(num_relations)\n",
    "        ])\n",
    "        # Relation-independent transform (optional)\n",
    "        self.self_weight = nn.Linear(in_features, out_features, bias=False)\n",
    "\n",
    "    def forward(self, x, adjacency_dict):\n",
    "        \"\"\"\n",
    "        x: (N, d_in)\n",
    "        adjacency_dict: {r: (N,N) adjacency matrix for relation r}\n",
    "        \"\"\"\n",
    "        out = torch.zeros(x.size(0), self.weight[0].out_features)\n",
    "        \n",
    "        for r in range(self.num_relations):\n",
    "            adj_r = adjacency_dict[r]\n",
    "            # possible normalization per relation\n",
    "            degree_r = torch.sum(adj_r, dim=1, keepdim=True)\n",
    "            degree_inv_r = 1.0 / (degree_r + 1e-6)\n",
    "            agg_r = torch.mm(adj_r, x) * degree_inv_r  # aggregator\n",
    "            out += self.weight[r](agg_r)\n",
    "\n",
    "        # Add self-connection\n",
    "        out += self.self_weight(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_relations):\n",
    "        super(RGCN, self).__init__()\n",
    "        self.rgcn1 = RGCNLayer(in_features, hidden_features, num_relations)\n",
    "        self.rgcn2 = RGCNLayer(hidden_features, out_features, num_relations)\n",
    "\n",
    "    def forward(self, x, adjacency_dict):\n",
    "        x = self.rgcn1(x, adjacency_dict)\n",
    "        x = F.relu(x)\n",
    "        x = self.rgcn2(x, adjacency_dict)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Example: Toy Multi-Relation Graph\n",
    "\n",
    "Let’s say we have 5 nodes and 2 different relations, each with its own adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-GCN output logits:\n",
      " tensor([[ 0.0488,  0.1305, -0.0340],\n",
      "        [ 0.0460,  0.1256, -0.0373],\n",
      "        [ 0.0484,  0.1276, -0.0355],\n",
      "        [ 0.0463,  0.1254, -0.0371],\n",
      "        [ 0.0477,  0.1271, -0.0362]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create random features\n",
    "x_toy_rgcn = torch.rand(N, d_in)\n",
    "\n",
    "# Build adjacency for 2 relations\n",
    "adj_rel0 = torch.zeros(N, N)\n",
    "adj_rel1 = torch.zeros(N, N)\n",
    "\n",
    "# Relation 0: connect i -> i+1 mod 5\n",
    "for i in range(N):\n",
    "    adj_rel0[i, (i+1) % N] = 1\n",
    "\n",
    "# Relation 1: connect i -> i+2 mod 5\n",
    "for i in range(N):\n",
    "    adj_rel1[i, (i+2) % N] = 1\n",
    "\n",
    "adj_dict = {\n",
    "    0: adj_rel0,\n",
    "    1: adj_rel1\n",
    "}\n",
    "\n",
    "# Instantiate RGCN\n",
    "model_rgcn = RGCN(in_features=d_in, hidden_features=8, out_features=3, num_relations=2)\n",
    "logits_rgcn = model_rgcn(x_toy_rgcn, adj_dict)\n",
    "print(\"R-GCN output logits:\\n\", logits_rgcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could then define labels for these nodes and train similarly with a chosen loss function. Each relation type will learn its own transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion & Further Reading\n",
    "\n",
    "In this tutorial, we covered:\n",
    "- **Graph Notation & Message Passing**\n",
    "- **Graph Convolutional Network (GCN)** theory + example\n",
    "- **Relational GCN (R-GCN)** for multi-relational data\n",
    "\n",
    "## Further Reading\n",
    "- Kipf & Welling (2017) [Semi-Supervised Classification with GCNs](https://arxiv.org/abs/1609.02907)\n",
    "- Schlichtkrull et al. (2018) [Modeling Relational Data with R-GCNs](https://arxiv.org/abs/1703.06103)\n",
    "- Velickovic et al. (2018) [Graph Attention Networks](https://arxiv.org/abs/1710.10903)\n",
    "- Hamilton et al. (2017) [GraphSAGE](https://arxiv.org/abs/1706.02216)\n",
    "- [PyTorch Geometric Documentation](https://pytorch-geometric.readthedocs.io/)\n",
    "- [DGL Documentation](https://www.dgl.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f95bd",
   "metadata": {},
   "source": [
    "# 7. Advanced Topics\n",
    "\n",
    "Some popular extensions to GCN/R-GCN include:\n",
    "1. **Graph Attention Networks (GAT)**: Use attention to weight neighbors differently.\n",
    "2. **GraphSAGE**: Sample fixed-size neighborhoods for large-scale graphs.\n",
    "3. **Heterogeneous Graphs**: Different node types and different edge types.\n",
    "4. **Scaling GNNs**: Efficient training on huge graphs (mini-batching, sampling, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "name": "01_GNN_Tutorial"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
